# useful to have this code snippet to avoid getting an error in case forgeting
# to close spark

try:
    spark.stop()
except:
    pass

# Using findspark to find automatically the spark folder
import findspark
findspark.init()

# import python libraries
import random

# initialize
from pyspark.sql import SparkSession


if __name__ == "__main__":
    spark = SparkSession \
            .builder \
            .appName("calculate_Pi") \
    	    .config("spark.driver.cores", 4) \
    	    .getOrCreate()


	num_samples = 100000000
 
	def inside(p):
	    x, y = random.random(), random.random()
	    return x*x + y*y < 1

	count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count()
	pi = 4 * count / num_samples
	print(pi)
